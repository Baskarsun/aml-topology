"""
Dashboard System Test Script

Verifies all components are working correctly before launching the full dashboard.
"""

import sys
import os
import time
import requests
from pathlib import Path

# Colors for terminal output
GREEN = '\033[92m'
RED = '\033[91m'
YELLOW = '\033[93m'
BLUE = '\033[94m'
RESET = '\033[0m'

def print_header(text):
    print(f"\n{BLUE}{'='*60}{RESET}")
    print(f"{BLUE}{text:^60}{RESET}")
    print(f"{BLUE}{'='*60}{RESET}\n")

def print_success(text):
    print(f"{GREEN}‚úÖ {text}{RESET}")

def print_error(text):
    print(f"{RED}‚ùå {text}{RESET}")

def print_warning(text):
    print(f"{YELLOW}‚ö†Ô∏è  {text}{RESET}")

def test_dependencies():
    """Test if all required packages are installed."""
    print_header("TEST 1: CHECKING DEPENDENCIES")
    
    packages = {
        'flask': 'Flask',
        'streamlit': 'Streamlit',
        'plotly': 'Plotly',
        'requests': 'Requests',
        'pandas': 'Pandas',
        'numpy': 'NumPy',
        'torch': 'PyTorch',
        'lightgbm': 'LightGBM'
    }
    
    all_ok = True
    for package, name in packages.items():
        try:
            __import__(package)
            print_success(f"{name:15} installed")
        except ImportError:
            print_error(f"{name:15} NOT FOUND")
            all_ok = False
    
    return all_ok

def test_models():
    """Test if models are trained and available."""
    print_header("TEST 2: CHECKING TRAINED MODELS")
    
    models_dir = Path('models')
    
    if not models_dir.exists():
        print_error("models/ directory not found")
        print_warning("Run: python main.py")
        return False
    
    required_files = {
        'lgb_model.txt': 'GBDT Model',
        'consolidation_config.json': 'Consolidator Config',
        'lstm_link_predictor.pt': 'LSTM Model'
    }
    
    all_ok = True
    for filename, description in required_files.items():
        filepath = models_dir / filename
        if filepath.exists():
            size_kb = filepath.stat().st_size / 1024
            print_success(f"{description:25} ({size_kb:.1f} KB)")
        else:
            print_error(f"{description:25} NOT FOUND")
            all_ok = False
    
    optional_files = {
        'gnn_model.pt': 'GNN Model',
        'sequence_detector_model.pt': 'Sequence Model'
    }
    
    for filename, description in optional_files.items():
        filepath = models_dir / filename
        if filepath.exists():
            size_kb = filepath.stat().st_size / 1024
            print_success(f"{description:25} ({size_kb:.1f} KB) [Optional]")
        else:
            print_warning(f"{description:25} Not found (optional)")
    
    return all_ok

def test_metrics_logger():
    """Test metrics logger functionality."""
    print_header("TEST 3: CHECKING METRICS LOGGER")
    
    try:
        sys.path.insert(0, os.getcwd())
        from src.metrics_logger import get_metrics_logger
        
        metrics = get_metrics_logger(db_path="test_metrics.db")
        print_success("MetricsLogger initialized")
        
        # Test logging
        metrics.log_inference({
            'timestamp': '2026-01-09T12:00:00',
            'account_id': 'TEST_001',
            'endpoint': '/test',
            'risk_score': 0.5,
            'risk_level': 'MEDIUM',
            'latency_ms': 25.0,
            'status': 'success'
        })
        print_success("Logged test inference")
        
        # Test retrieval
        recent = metrics.get_recent_inferences(limit=1)
        if recent:
            print_success("Retrieved inference logs")
        else:
            print_warning("No logs retrieved (expected for fresh DB)")
        
        # Clean up
        if Path("test_metrics.db").exists():
            Path("test_metrics.db").unlink()
            print_success("Test database cleaned up")
        
        return True
    except Exception as e:
        print_error(f"MetricsLogger test failed: {e}")
        return False

def test_inference_engine():
    """Test inference engine loading."""
    print_header("TEST 4: CHECKING INFERENCE ENGINE")
    
    try:
        sys.path.insert(0, os.getcwd())
        from src.inference_api import InferenceEngine
        
        print("Loading inference engine...")
        engine = InferenceEngine()
        
        # Check loaded models
        for model_name, model_obj in engine.models.items():
            if model_obj is not None:
                print_success(f"{model_name.upper():15} loaded")
            else:
                print_warning(f"{model_name.upper():15} not loaded (may be optional)")
        
        if engine.consolidator:
            print_success("CONSOLIDATOR   loaded")
        else:
            print_error("CONSOLIDATOR   not loaded")
            return False
        
        return True
    except Exception as e:
        print_error(f"Inference engine test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_api_connection():
    """Test if API is running."""
    print_header("TEST 5: CHECKING API CONNECTION")
    
    try:
        response = requests.get('http://localhost:5000/health', timeout=2)
        if response.status_code == 200:
            data = response.json()
            print_success("API is running")
            print(f"         Status: {data.get('status')}")
            return True
        else:
            print_warning("API returned non-200 status")
            return False
    except requests.exceptions.ConnectionError:
        print_warning("API not running (start with: python -m src.inference_api)")
        return False
    except Exception as e:
        print_error(f"API test failed: {e}")
        return False

def test_transaction_simulation():
    """Test sending a transaction to the API."""
    print_header("TEST 6: TESTING TRANSACTION SCORING")
    
    try:
        response = requests.get('http://localhost:5000/health', timeout=2)
        if response.status_code != 200:
            print_warning("API not running - skipping transaction test")
            return False
    except:
        print_warning("API not running - skipping transaction test")
        return False
    
    # Send test transaction
    test_payload = {
        "account_id": "TEST_ACC_001",
        "transaction": {
            "amount": 1500.0,
            "mcc": "5411",
            "payment_type": "card",
            "device_change": False,
            "ip_risk": 0.2,
            "count_1h": 2,
            "sum_24h": 3000.0,
            "uniq_payees_24h": 3,
            "is_international": False,
            "avg_tx_24h": 1000.0,
            "velocity_score": 0.3
        },
        "events": ["login_success", "view_account", "transfer", "logout"]
    }
    
    try:
        response = requests.post(
            'http://localhost:5000/score/consolidate',
            json=test_payload,
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            print_success("Transaction scored successfully")
            print(f"         Risk Score: {result.get('consolidated_risk_score', 'N/A')}")
            print(f"         Risk Level: {result.get('risk_level', 'N/A')}")
            return True
        else:
            print_error(f"API returned error: {response.status_code}")
            return False
    except Exception as e:
        print_error(f"Transaction test failed: {e}")
        return False

def test_dashboard_imports():
    """Test if dashboard can be imported."""
    print_header("TEST 7: CHECKING DASHBOARD IMPORTS")
    
    try:
        import streamlit as st
        print_success("Streamlit imported")
        
        import plotly.express as px
        print_success("Plotly imported")
        
        # Try to import dashboard module (won't run, just check imports)
        sys.path.insert(0, os.getcwd())
        # Don't actually run the dashboard, just check it can be imported
        print_success("Dashboard code structure valid")
        
        return True
    except Exception as e:
        print_error(f"Dashboard import test failed: {e}")
        return False

def main():
    """Run all tests."""
    print_header("üß™ AML DASHBOARD SYSTEM TEST")
    print("This script verifies all components are working correctly.\n")
    
    results = {
        'Dependencies': test_dependencies(),
        'Models': test_models(),
        'Metrics Logger': test_metrics_logger(),
        'Inference Engine': test_inference_engine(),
        'API Connection': test_api_connection(),
        'Transaction Scoring': test_transaction_simulation(),
        'Dashboard Imports': test_dashboard_imports()
    }
    
    # Summary
    print_header("TEST SUMMARY")
    
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    
    for test_name, passed_test in results.items():
        status = f"{GREEN}PASS{RESET}" if passed_test else f"{RED}FAIL{RESET}"
        print(f"{test_name:25} {status}")
    
    print()
    print(f"Results: {passed}/{total} tests passed")
    
    if passed == total:
        print()
        print_success("All tests passed! System is ready.")
        print()
        print("Next steps:")
        print("  1. Start API:       python -m src.inference_api")
        print("  2. Start Simulator: python transaction_simulator.py")
        print("  3. Start Dashboard: streamlit run dashboard.py")
        print()
        print("Or use one-command launcher: python launch_dashboard.py")
    else:
        print()
        print_error("Some tests failed. Please fix issues before launching.")
        print()
        print("Common fixes:")
        print("  - Install missing packages: pip install <package>")
        print("  - Train models: python main.py")
        print("  - Start API: python -m src.inference_api")
    
    print()
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
